{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textsimilarity\n",
    "\n",
    "Examples of using the textsimilarity package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Get text similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relaxing vacation',\n",
       " 'girls night out',\n",
       " 'wedding party',\n",
       " 'bridal flowers',\n",
       " 'soccer game',\n",
       " 'skate park']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os, sys\n",
    "\n",
    "#load example text corpus for comparison\n",
    "input_data = pd.read_csv(os.path.join(sys.path[0], 'comparison_corpus.csv'))\n",
    "comparison_corpus = input_data['phrases_to_compare'].values.tolist()\n",
    "comparison_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from textsimilarity import text_models, rankers\n",
    "\n",
    "#load a text model to use for generating text embeddings\n",
    "bert_model = text_models.BertBaseModel()    \n",
    "\n",
    "#specify which model and corpus to use for comparison\n",
    "cosine_sim_ranker = rankers.CosineSimilarityRanker(\n",
    "                    bert_model, \n",
    "                    comparison_corpus\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Target text phrase:  massage\n",
      "Most similar text phrase from corpus:  relaxing vacation\n",
      "Cosine similarity score:  0.9378038048744202 \n",
      "------\n",
      "------\n",
      "Target text phrase:  football\n",
      "Most similar text phrase from corpus:  soccer game\n",
      "Cosine similarity score:  0.9419365525245667 \n",
      "------\n",
      "------\n",
      "Target text phrase:  martini and chocolate\n",
      "Most similar text phrase from corpus:  girls night out\n",
      "Cosine similarity score:  0.955855667591095 \n",
      "------\n"
     ]
    }
   ],
   "source": [
    "def print_closest_result(target, ranker):\n",
    "    closest_phrase_cos_sim = ranker.rank_on_similarity(target)[0]\n",
    "    print('------\\nTarget text phrase: ', target)\n",
    "    print('Most similar text phrase from corpus: ', closest_phrase_cos_sim[0])\n",
    "    print('Cosine similarity score: ', closest_phrase_cos_sim[1], '\\n------')\n",
    "\n",
    "#print text phrases and cosine similarity scores\n",
    "print_closest_result('massage', cosine_sim_ranker)\n",
    "print_closest_result('football', cosine_sim_ranker)\n",
    "print_closest_result('martini and chocolate', cosine_sim_ranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Using spell check before text similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relaxin vacatin',\n",
       " 'girlz night outt',\n",
       " 'weddding party',\n",
       " 'bridal flowera',\n",
       " 'sokcer game',\n",
       " 'skate parkk']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example corpus with some spelling mistakes\n",
    "corpus_with_misspelling = input_data['misspelled_phrases'].values.tolist()\n",
    "corpus_with_misspelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relax vacation',\n",
       " 'girl night out',\n",
       " 'wedding party',\n",
       " 'bridal flower',\n",
       " 'soccer game',\n",
       " 'skate park']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textsimilarity import clean_text\n",
    "\n",
    "text_cleaner = clean_text.CleanText()\n",
    "\n",
    "#correct spelling errors\n",
    "spell_checked_phrases = []\n",
    "for phrase in corpus_with_misspelling:\n",
    "    corrected_phrase = text_cleaner.spelling_correction(phrase)\n",
    "    spell_checked_phrases.append(corrected_phrase)\n",
    "spell_checked_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify which model and corpus to use for comparison\n",
    "cosine_sim_ranker = rankers.CosineSimilarityRanker(\n",
    "                            bert_model, \n",
    "                            spell_checked_phrases\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Target text phrase:  massage\n",
      "Most similar text phrase from corpus:  relax vacation\n",
      "Cosine similarity score:  0.9411255717277527 \n",
      "------\n",
      "------\n",
      "Target text phrase:  football\n",
      "Most similar text phrase from corpus:  soccer game\n",
      "Cosine similarity score:  0.9419365525245667 \n",
      "------\n",
      "------\n",
      "Target text phrase:  martini and chocolate\n",
      "Most similar text phrase from corpus:  girl night out\n",
      "Cosine similarity score:  0.9597087502479553 \n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#print text phrases and cosine similarity scores\n",
    "print_closest_result('massage', cosine_sim_ranker)\n",
    "print_closest_result('football', cosine_sim_ranker)\n",
    "print_closest_result('martini and chocolate', cosine_sim_ranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using the profanity filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relaxing vacation',\n",
       " 'girls night out',\n",
       " 'wedding party',\n",
       " 'bridal flowers',\n",
       " 'soccer game',\n",
       " 'skate park',\n",
       " 'go to hell']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example corpus containing a phrase with profanity\n",
    "profane_text_phrase = 'go to hell'\n",
    "comparison_corpus.append(profane_text_phrase)\n",
    "comparison_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relaxing vacation',\n",
       " 'girls night out',\n",
       " 'wedding party',\n",
       " 'bridal flowers',\n",
       " 'soccer game',\n",
       " 'skate park']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove phrases that contain profanity\n",
    "for phrase in comparison_corpus:\n",
    "    is_profane = text_cleaner.determine_text_profanity(phrase)\n",
    "    if is_profane:\n",
    "        comparison_corpus.remove(phrase)\n",
    "comparison_corpus"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c3adbe5a260b41ba9147af0d6966a33650f1ad814856e01b4b7c66ad6f6ef6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('data515': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
